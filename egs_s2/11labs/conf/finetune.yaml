# config for LibriLight 6k (small + medium)
# 30k_basex1_hubert_L7km300
model:
    target: soundstorm.s2.models.dalle_wav.dalle_wav.DALLE
    params:
        content_info: {key: audio}
        condition_info: {key: text}
        n_q: 4  # the encodec codebook's number, same as `num_quant` of dataloader
        diffusion_config:
            target: soundstorm.s2.models.dalle_wav.diffusion_transformer.DiffusionTransformer
            params:
                diffusion_step: 100
                alpha_init_type: 'alpha1'       # init_type = fix or cos or linear
                auxiliary_loss_weight: 5.0e-4
                adaptive_auxiliary_loss: True
                mask_weight: [1, 1]    # the loss weight on mask region and non-mask region
                transformer_config:
                    target: soundstorm.s2.models.dalle_wav.transformer_utils.Text2ImageTransformer
                    params:
                        attn_type: 'selfcross' # using self attention
                        n_layer: 16 # we may use large model
                        n_embd: 512 # the dim of embedding dims
                        condition_dim: 512
                        n_head: 8
                        attn_pdrop: 0.0
                        resid_pdrop: 0.0
                        block_activate: GELU2
                        timestep_type: 'adalayernorm'   # adainsnorm or adalayernorm and abs
                        mlp_hidden_times: 4
                        semantic_token_nums: 300
                        prompt_semantic_emb_len: 10 # should > max_prompt_sec in dataset
                        target_semantic_emb_len: 30 # should > max_target_sec in dataset
                        prompt_acoustic_emb_len: 10 # can be same with prompt_semantic
                        target_acoustic_emb_len: 30 # can be same with target_semantic
                        content_emb_config:
                            target: soundstorm.s2.models.dalle_wav.mask_embedding.DalleMaskImageEmbedding
                            params:
                                num_embed: 1026 # should be quantize_number
                                max_size: 16000
                                embed_dim: 512  # the dim of postion embedding
                                trainable: True
                                pos_emb_type: embedding

solver:
    base_lr: 0.3e-05  # 3.0e-6 x 8 cause max_token_one_batch is x 8 (the old is 10k)
    adjust_lr: none   # not adjust lr according to total batch_size
    max_iters: 650000 # 比 60k_default.yaml 里面的 60w iter 大一些
    save_iters: 1500  # 1.5k, ~ cost 0.3h to save a ckpt
    dev_iters: 1500   # num of iter for each gpu, for 8 gpu here, should x2 when use 4 gpus to make model see same number of samples
    ema:
        decay: 0.99
        update_interval: 25
        device: cpu
    clip_grad_norm:
        target: soundstorm.s2.engine.clip_grad_norm.ClipGradNorm
        params:
            start_iteration: 0
            end_iteration: 5000
            max_norm: 0.5
    optimizers_and_schedulers:  # a list of configures, so we can config several optimizers and schedulers
      - name: none  # default is None
        optimizer:
            target: torch.optim.AdamW
            params:
                betas: !!python/tuple [0.9, 0.96]
                weight_decay: 1.0e-2
        scheduler:
            step_iteration: 1
            target: soundstorm.s2.engine.lr_scheduler.ReduceLROnPlateauWithWarmup
            params:
                factor: 0.5
                patience: 25000
                min_lr: 1.0e-06
                threshold: 1.0e-1
                threshold_mode: rel
                warmup_lr: 0.45e-3   # the lr to be touched after warmup
                warmup: 800  # num of iter to warmup

dataloader:
    max_token_one_batch: 30000 # 11labs 里面有大于 20s 的（LibriLight VAD 最大 20, dataloader 最大用到 23s）, 所以这里要适当减小
    num_workers: 4
    prefetch_factor: 50
    train_datasets: # a list of configures, so we can combine several schedulers
        - target: soundstorm.s2.data.semantic_dataset_librilight_60k.SemanticDataset
          params:
                codec_name: hificodec
                num_quant: 4    # not work when != 4 for hificodec, and can be 3 for soundstream and encodec
                semantic_token_nums: 300 # same with num of kmeans bins
                max_prompt_sec: 3        # be same with LibriTTS
                max_target_sec: 20       # LibriLight 中真正有用的是 17, 否则 position emb 可能报错
                semantic_paths: ['dump/small/train/semantic_token_0_3.tsv', 'dump/small/train/semantic_token_1_3.tsv']
                acoustic_paths: ['dump/small/train/acoustic_token/hificodec_0_3.pth', 'dump/small/train/acoustic_token/hificodec_2_3.pth']

    dev_datasets:
        - target: soundstorm.s2.data.semantic_dataset_librilight_60k.SemanticDataset
          params:
                codec_name: hificodec
                num_quant: 4    # not work when != 4 for hificodec, and can be 3 for soundstream and encodec
                semantic_token_nums: 300 # same with num of kmeans bins
                max_prompt_sec: 3        # be same with LibriTTS
                max_target_sec: 20       # LibriLight 中真正有用的是 17
                semantic_paths: ['dump/small/dev/semantic_token_0_3.tsv', 'dump/small/dev/semantic_token_1_3.tsv']
                acoustic_paths: ['dump/small/dev/acoustic_token/hificodec_0_3.pth', 'dump/small/dev/acoustic_token/hificodec_2_3.pth']
